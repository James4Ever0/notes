---
title: 'Unsloth, training quantized model'
created: '2025-01-28T09:25:26.117Z'
modified: '2025-01-29T14:44:47.332Z'
---

# Unsloth, training quantized model

declarative ai:

https://ludwig.ai/latest/

---

ollama importing from unsloth:

https://github.com/ollama/ollama/blob/main/docs/import.md

---

fine-tuning quantized models:

https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama

https://github.com/hiyouga/LLaMA-Factory

https://github.com/artidoro/qlora

https://predibase.com/blog/lora-land-fine-tuned-open-source-llms-that-outperform-gpt-4

---

multi-lora serving

https://github.com/predibase/lorax

https://github.com/punica-ai/punica

---

other quantization methods:

https://github.com/ModelCloud/GPTQModel

---

quantization method comparison:

https://standardscaler.com/2024/03/09/navigating-the-jungle-of-llm-quantization-formats-gguf-gptq-awq-and-exl2-which-one-to-pick/

https://www.theregister.com/2024/07/14/quantization_llm_feature/

https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/

https://www.maartengrootendorst.com/blog/quantization/

https://newsletter.maartengrootendorst.com/p/which-quantization-method-is-right

