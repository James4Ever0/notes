---
title: Serve Models from Replicate Locally
created: '2024-03-01T01:42:44.579Z'
modified: '2024-03-01T01:47:41.427Z'
---

# Serve Models from Replicate Locally

Replicate internally use [Cog](https://github.com/replicate/cog) for packaging and serving large AI models in Docker containers. Currently it only supports macOS and Linux.

According to the [doc](https://cog.run/) it offers nearly the same functionally as Replicate such as API calls, fine-tuning.

---

You may connect your local LLM to VSCode using [Continue](https://continue.dev), an open-source Copilot alternative.
